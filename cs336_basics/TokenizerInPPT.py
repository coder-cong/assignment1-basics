import tiktoken
from abc import ABC
from dataclasses import dataclass
from collections import defaultdict
import os


# å®šä¹‰tokenizeræ¥å£
class Tokenizer(ABC):
    def encode(self, string: str) -> list[int]:
        raise NotImplementedError

    def decode(self, indices: list[int]) -> str:
        raise NotImplementedError


class CharacterTokenizer(Tokenizer):
    # åŸºäºå­—ç¬¦åˆ†è¯ï¼Œåˆ©ç”¨mapå¯¹æ¯ä¸ªå­—ç¬¦è¿›è¡Œæ“ä½œ
    def encode(self, string: str) -> list[int]:
        indices = list(map(ord, string))
        return indices

    def decode(self, indices: list[int]) -> str:
        # ä½¿ç”¨ç©ºå­—ç¬¦ä¸².join(list)å®Œæˆæ‹¼æ¥å­—ç¬¦ä¸²
        return "".join(map(chr, indices))


class ByteTokenizer(Tokenizer):
    def encode(self, string: str) -> list[int]:
        # string encodeå¾—åˆ°å­—èŠ‚æ•°ç»„
        return list(map(int, string.encode()))

    def decode(self, indices: list[int]) -> str:
        #  bytes(list)å¯ä»¥å°†listä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ è½¬æ¢ä¸ºå­—èŠ‚
        string_bytes = bytes(indices)
        # å­—èŠ‚æ•°ç»„ decodeå¯ä»¥å¾—åˆ°å­—ç¬¦ä¸²
        string = string_bytes.decode("utf-8")
        return string


def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:
    # TODO æµ‹è¯•mergeåŠŸèƒ½
    # å°†åŸåºåˆ—indicesä¸­çš„pairåˆå¹¶ä¸ºnew_index
    new_indices = []
    index = 0
    while index < len(indices):
        if index < len(indices)-1 and indices[index] == pair[0] and indices[index+1] == pair[1]:
            new_indices.append(new_index)
            index += 2
        else:
            new_indices.append(indices[index])
            index += 1
    return new_indices


@dataclass
class BytePairEncodingParam:
    vocab: dict[int, bytes]  # index -> bytes
    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index


class BytePairEncodingTokenizer(Tokenizer):
    def __init__(self, param: BytePairEncodingParam):
        self.param = param

    def encode(self, string: str) -> list[int]:
        # TODO æµ‹è¯•ç¼–ç åŠŸèƒ½
        indices = list(map(int, string.encode()))
        for pair, new_index in self.param.merges.items():
            indices = merge(indices, pair, new_index)
        return indices

    def decode(self, indices: list[int]) -> str:
        bytes_list = list(map(self.param.vocab.get, indices))
        return b"".join(bytes_list).decode()


def trainBPE_origin(string: str, merge_num: int) -> BytePairEncodingParam:
    # è¿™ç§æ–¹å¼æ¯æ¬¡mergeéƒ½éœ€è¦å®Œæ•´éå†ä¸€éè¯­æ–™åº“ï¼Œæ•ˆç‡è¾ƒä½
    indices = list(map(int, string.encode("utf-8")))
    # å®šä¹‰vocabå’Œmergesï¼Œvocabå®šä¹‰äº†intåˆ°bytesçš„è½¬æ¢
    vocab: dict[int, bytes] = {i: bytes([i]) for i in range(256)}
    merges: dict[tuple[int, int], int] = {}
    for i in range(merge_num):
        # 1. æ¯æ¬¡mergeæ‰¾å‡ºé¢‘ç‡æœ€é«˜çš„ä¸€å¯¹index
        counts = defaultdict(int)
        pairs = list(zip(indices, indices[1:]))
        for pair in pairs:
            counts[pair] += 1
        max_pair = max(pairs, key=counts.get)
        # 2. é¢‘ç‡æœ€é«˜çš„ä¸€å¯¹indexåŠ å…¥å­—å…¸ä¸­
        new_index = 256+i
        merges[max_pair] = new_index
        vocab[new_index] = vocab[max_pair[0]]+vocab[max_pair[1]]
        # 3. å®Œæˆä¸€æ¬¡mergeæ“ä½œ
        indices = merge(indices, max_pair, new_index)
    return BytePairEncodingParam(vocab=vocab, merges=merges)


def byteTokenizerTest():
    string = "h e l l o hellohelloğŸ˜Šworld"
    byteTokenizer = ByteTokenizer()
    encoded = byteTokenizer.encode(string)
    print(encoded)
    decoded = byteTokenizer.decode(encoded)
    print(decoded)
    print(string == decoded)


def characterTokenizerTest():
    string = "helloğŸ˜Šworld"
    characterTokenizer = CharacterTokenizer()
    encoded = characterTokenizer.encode(string)
    print(encoded)
    decoded = characterTokenizer.decode(encoded)
    print(decoded)
    print(string == decoded)


def BPETokenizerTest():
    # æµ‹è¯•bpeè®­ç»ƒ
    # 1. è¯»å–æ–‡æœ¬æ–‡ä»¶ä¸ºå­—ç¬¦ä¸²
    # 2. è°ƒç”¨è®­ç»ƒå‡½æ•°
    # 3. æ‰“å°æŸ¥çœ‹è®­ç»ƒå‚æ•°ç»“æœ
    # 4. æ‰“æ–­ç‚¹æŸ¥çœ‹æƒ…å†µ
    text = open("C:/Projs/assignment1-basics/tests/fixtures/tinystories_sample.txt",
                encoding="utf-8").read()
    # ä½¿ç”¨ä¸€ä¸ªç®€å•å­—ç¬¦ä¸²æµ‹è¯•
    param = trainBPE_origin(text, 10)
    tokenizer = BytePairEncodingTokenizer(param)
    print(param.vocab)
    print(param.merges)


def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
    return "".join([bytes([b]).decode("utf-8") for b in bytestring])


if __name__ == "__main__":
    # characterTokenizerTest()
    # byteTokenizerTest()
    BPETokenizerTest()

    # BPETrainExample()
    # trainBPE("C:/Projs/assignment1-basics/tests/fixtures/tinystories_sample.txt",
    #          vocab_size=15000, special_tokens=["<|endoftext|>"])
