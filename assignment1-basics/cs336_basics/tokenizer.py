# å¯¼å…¥ Self ç”¨äºç±»å‹æç¤ºï¼ŒList ç”¨äºæ¸…æ™°è¡¨ç¤ºåˆ—è¡¨ç±»å‹
from typing import Self, Optional, List, Iterable, Iterator, BinaryIO
import tiktoken
from abc import ABC
from dataclasses import dataclass, field
from collections import defaultdict
import os
import time
import heapq
import regex as re
from multiprocessing import Process, Queue, Pool
import logging
# é…ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
logging.basicConfig(
    level=logging.INFO,  # è®¾ç½®æœ€ä½çº§åˆ«ä¸º INFO
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log"),  # æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶
        logging.StreamHandler()  # æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)
# è·å–ä¸€ä¸ªLoggerå®ä¾‹ (æ¨èä½¿ç”¨ __name__ æ¥åŒºåˆ†ä¸åŒæ¨¡å—çš„Logger)
logger = logging.getLogger(__name__)


def find_chunk_boundaries(
    file: BinaryIO,
    desired_num_chunks: int,
    split_special_token: bytes,
) -> list[int]:
    """
    Chunk the file into parts that can be counted independently.
    May return fewer chunks if the boundaries end up overlapping.
    e.g. å®é™…çš„åˆ†ç•Œç‚¹åœ¨12 20 åˆ†å—çš„ç‚¹ä¸º5 10 15 20 ç¬¬ä¸€å—å’Œç¬¬äºŒå—æœ‰é‡å ï¼Œè¿”å›äº†ç›¸åŒçš„åˆ†ç•Œç‚¹ï¼Œç»è¿‡æ’åºå»é‡ä¹‹åç»“æœä¼šå°‘
    """
    assert isinstance(split_special_token,
                      bytes), "Must represent special token as a bytestring"

    # Get total file size in bytes
    file.seek(0, os.SEEK_END)
    file_size = file.tell()
    file.seek(0)

    chunk_size = file_size // desired_num_chunks

    # Initial guesses for chunk boundary locations, uniformly spaced
    # Chunks start on previous index, don't include last index
    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]
    chunk_boundaries[-1] = file_size

    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time

    for bi in range(1, len(chunk_boundaries) - 1):
        initial_position = chunk_boundaries[bi]
        file.seek(initial_position)  # Start at boundary guess
        while True:
            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk

            # If EOF, this boundary should be at the end of the file
            if mini_chunk == b"":
                chunk_boundaries[bi] = file_size
                break

            # Find the special token in the mini chunk
            found_at = mini_chunk.find(split_special_token)
            if found_at != -1:
                chunk_boundaries[bi] = initial_position + found_at
                break
            initial_position += mini_chunk_size

    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks
    return sorted(set(chunk_boundaries))


def BPE_train_example():

    def merge(word_freq: dict[tuple, int], pair: tuple[bytes, bytes], new_token: bytes):
        # éå†æ¯ä¸ªwordï¼Œè¿›è¡Œmerge
        new_word_freq = defaultdict(int)
        for word, freq in word_freq.items():
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word)-1 and word[i] == pair[0] and word[i+1] == pair[1]:
                    new_word.append(new_token)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word_freq[tuple(new_word)] = freq
        return new_word_freq

        # åœ¨ä¸€ä¸ªç®€å•çš„ä¾‹å­ä¸Šå®ŒæˆBPEè®­ç»ƒ
    text = """low low low low low
lower lower widest widest widest
newest newest newest newest newest newest"""

    # 1. åˆå§‹åŒ–è¯è¡¨
    vocab: dict[int, bytes] = {}
    merges: list[tuple[bytes, bytes]] = []
    vocab[0] = "<|endoftext|>".encode("utf-8")
    vocab.update(dict({1+i: bytes([i]) for i in range(256)}))
    # 2. é¢„åˆ†è¯ï¼Œä½¿ç”¨ç©ºç™½å­—ç¬¦åˆ†è¯è·å–è¯é¢‘ç‡è¡¨ï¼Œè¯ä»¥byteså…ƒç»„è¡¨ç¤º
    words = text.split()
    word_freq = defaultdict(int)
    for word in words:
        word_freq[tuple((bytes([byte]) for byte in word.encode("utf-8")))] += 1
    # 3. è¿›è¡Œmerge
    num_merges = 6
    near_bytes_freq = defaultdict(int)
    vocab_len = len(vocab)
    for i in range(num_merges):
        # 3.1 éå†å…¨éƒ¨è¯ï¼Œæ‰¾åˆ°ç›¸é‚»ä¸¤ä¸ªå­—èŠ‚å‡ºç°æ¬¡æ•°æœ€å¤šçš„
        for word, freq in word_freq.items():
            for byte1, byte2 in zip(word, word[1:]):
                # è¿™é‡Œä¸èƒ½å°†ä¸¤ä¸ªbytesæ‹¼æ¥ä¹‹åæ”¾è¿›å­—å…¸ï¼Œå› ä¸ºä¸çŸ¥é“æ˜¯ä¸¤ä¸ªbytesæ˜¯æ€æ ·çš„
                near_bytes_freq[(byte1, byte2)] += freq
        # 3.2 å‡ºç°æ¬¡æ•°æœ€å¤šçš„é€‰æ‹©å‡ºæ¥ä½œä¸ºnew_tokenï¼ŒåŠ å…¥åˆ°è¯æ±‡è¡¨å’Œmergesä¸­ï¼Œéšåè¿›è¡Œmerge
        # maxå‡½æ•°keyè¿”å›ä¸€ä¸ªå…ƒç»„maxå‡½æ•°ä¼šä¾æ¬¡æ¯”è¾ƒå…ƒç»„é‡Œé¢çš„æ¯ä¸ªå…ƒç´ å¯¹åº”çš„å€¼
        max_bytes = max(near_bytes_freq.keys(),
                        key=lambda key: (near_bytes_freq.get(key), key[0]+key[1]))
        new_token = max_bytes[0]+max_bytes[1]
        vocab[vocab_len+i] = new_token
        pair = (max_bytes[0], max_bytes[1])
        merges.append(pair)
        word_freq = merge(word_freq, pair, new_token)
        near_bytes_freq.clear()

    print(f"vocab:{vocab}")
    print(f"merges:{merges}")


@dataclass
class LinkedNode:
    value: bytes = None
    token_id: int = 0
    preNode: Self = None
    nextNode: Self = None

    def __str__(self):
        node = self
        nodeStr = b""
        while node is not None:
            nodeStr += node.value
            nodeStr += b"-"
            node = node.nextNode
        return nodeStr.decode("utf-8")[:-1]


@dataclass
class PairInStr:
    string: str = None
    # countç”¨äºè®¡æ•°è¿™ä¸ªpairåœ¨strä¸­å‡ºç°äº†å¤šå°‘æ¬¡ï¼Œå¦‚æœæ¬¡æ•°ä¸º0é‚£ä¹ˆéœ€è¦è·³è¿‡
    count: int = 0

    def __hash__(self) -> int:
        return self.string.__hash__()

    def __eq__(self, other) -> bool:
        return self.string == other.string


@dataclass
class PairItem:
    """_summary_
    å°è£…äº†å­—èŠ‚å¯¹ï¼Œå­—èŠ‚å¯¹æ•°é‡ä»¥åŠå­—èŠ‚å¯¹æ¯”è¾ƒæ–¹å¼ï¼Œæ–¹ä¾¿ä½¿ç”¨å †å–å‡ºæœ€å¤§çš„å…ƒç´ 
    """
    pair: tuple[bytes, bytes] = None
    count: int = 0

    def __lt__(self, other):
        # ä¼˜å…ˆæ¯”è¾ƒcountæ•°é‡,ç”±äºpythonä¸­çš„heapqæ˜¯å°é¡¶å †ï¼Œéœ€è¦æŠŠåˆ¤æ–­ç¿»è½¬
        if self.count != other.count:
            return self.count > other.count
        # countæ•°é‡ç›¸åŒæ¯”è¾ƒä¸¤ä¸ªå­—èŠ‚çš„å­—å…¸åº
        if self.pair[0] != other.pair[0]:
            return self.pair[0] > other.pair[0]
        return self.pair[1] > other.pair[1]

    def __eq__(self, other):
        return self.count == other.count and self.pair[0] == other.pair[0] and self.pair[1] == other.pair[1]


@dataclass
class ProcessMessageBlock:
    # å­è¿›ç¨‹ç¼–å·
    idx: int = 0
    # å­è¿›ç¨‹å¤„ç†æ—¶é—´
    process_time: float = 0.0
    # å­è¿›ç¨‹æ¶ˆæ¯
    msg: str = ""
    # å­è¿›ç¨‹å¤„ç†åŒºé—´èµ·ç‚¹
    start: Optional[int] = None
    # å­è¿›ç¨‹å¤„ç†åŒºé—´ç»ˆç‚¹
    end: Optional[int] = None
    # pretokenizeè¿”å›å¤šä¸ªåˆ†å—çš„é¢„åˆ†è¯ç»“æœ
    pretokenized_chunks: defaultdict = None
    # BPE Train mergeè¿‡ç¨‹å­è¿›ç¨‹è¿”å›å¤šä¸ªåˆ†å—çš„mergeç»“æœ
    merged_chunks: List[defaultdict] = None
    # mergeè¿‡ç¨‹ä¸­ç»Ÿè®¡ç›¸é‚»å­—èŠ‚å¯¹å‡ºç°æ•°æ®
    near_bytes: Optional[defaultdict] = None
    # mergeè¿‡ç¨‹ä¸­ç»Ÿè®¡æ–°tokenç›¸é‚»å­—èŠ‚å¯¹æ•°é‡
    new_token_freq: Optional[defaultdict] = None

    def set_msg(self, msg: str) -> Self:
        """è®¾ç½®æ¶ˆæ¯ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.msg = msg  # ç›´æ¥èµ‹å€¼æ¯” __setattr__ æ›´å¸¸ç”¨å’Œæ¨è
        return self

    def set_process_time(self, process_time: float) -> Self:
        """å­è¿›ç¨‹å®Œæˆä»»åŠ¡éœ€è¦æ—¶é—´ï¼Œè¿›è¡Œæ€§èƒ½åˆ†æ"""
        self.process_time = process_time
        return self

    def set_pretokenized_chunks(self, pretokenized_chunks: defaultdict) -> Self:
        """è®¾ç½®é¢„åˆ†è¯å—ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.pretokenized_chunks = pretokenized_chunks
        return self

    def set_idx(self, idx: int) -> Self:
        """è®¾ç½®å­è¿›ç¨‹ç¼–å·ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.idx = idx
        return self

    def set_start(self, start: int) -> Self:
        """è®¾ç½®å¤„ç†åŒºé—´èµ·ç‚¹ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.start = start
        return self

    def set_end(self, end: int) -> Self:
        """è®¾ç½®å¤„ç†åŒºé—´ç»ˆç‚¹ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.end = end
        return self

    def set_merged_chunks(self, merged_chunks: List[defaultdict]) -> Self:
        """è®¾ç½®åˆå¹¶åçš„åˆ†å—ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.merged_chunks = merged_chunks
        return self

    def set_near_bytes(self, near_bytes: defaultdict) -> Self:
        """è®¾ç½®ç›¸é‚»å­—èŠ‚å¯¹ç»Ÿè®¡æ•°æ®ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.near_bytes = near_bytes
        return self

    def set_new_token_freq(self, new_token_freq: defaultdict) -> Self:
        """è®¾ç½®æ–°tokenç›¸é‚»å­—èŠ‚å¯¹ç»Ÿè®¡æ•°æ®ï¼Œå¹¶è¿”å›è‡ªèº«ä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚"""
        self.new_token_freq = new_token_freq
        return self


def train_chunk_pretokenize(input_path: str, idx: int, blockes: list[int],  special_tokens) -> list[defaultdict]:
    """_summary_

    Args:
        idx (int): å­è¿›ç¨‹ç¼–å·
        blockes (list[int]): å­ç¨‹éœ€è¦å¤„ç†çš„å—
        queue (Queue): ä¸»è¿›ç¨‹ä¸å­è¿›ç¨‹é—´é€šä¿¡çš„æ¶ˆæ¯é˜Ÿåˆ—ï¼Œå­è¿›ç¨‹åªéœ€è¦å‘é˜Ÿåˆ—ä¸­æ’å…¥æ•°æ®
    """
    message_block = ProcessMessageBlock(idx=idx)
    try:
        if blockes is None:
            msg = f"worker-{idx} éœ€è¦å¤„ç†çš„å—ä¸ºç©ºï¼Œé€€å‡ºå¤„ç†"
            return message_block.set_msg(msg)
        # logger.info(f"worker-{idx} æ¥æ”¶åˆ°ä»»åŠ¡ï¼Œå¤„ç†ï¼š{blockes}")
        with open(input_path, "rb") as f:
            # 1. æ¯ä¸ªè¿›ç¨‹åªéœ€è¦å¤„ç†ä¸€ä¸ªå—ï¼Œéœ€è¦é¦–å…ˆä½¿ç”¨special_tokenså¯¹æ–‡æœ¬è¿›è¡Œåˆ†å‰²æˆå¤šæ®µ
            f.seek(blockes[0])
            text_to_split = f.read(blockes[1]-blockes[0]).decode("utf-8")
            blocks = re.split(
                re.escape("|".join(special_tokens)), text_to_split)
            if len(blocks) == 0:
                msg = f"worker-{idx} éœ€è¦å¤„ç†çš„å—ä¸ºç©ºï¼Œé€€å‡ºå¤„ç†"
                return message_block.set_msg(msg)
            # 2. åˆ†å‰²æˆå¤šæ®µï¼Œå¯¹æ¯æ®µè¿›è¡Œé¢„åˆ†è¯ï¼Œæ¯æ®µå•ç‹¬ä¿å­˜
            multi_block_word_freq = defaultdict(int)
            for block in blocks:
                for word in re.finditer(
                        r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""", block):
                    # éœ€è¦åŠ ä¸Šå»æ‰\rï¼Œå¦åˆ™ä¼šå‡ºç°é—®é¢˜
                    multi_block_word_freq[word.group(0).replace('\r', '')] += 1
            msg = "å¤„ç†æˆåŠŸ"
            message_block.set_msg(
                msg).set_pretokenized_chunks(multi_block_word_freq)
    except Exception as e:
        msg = f"å¤„ç†è¿‡ç¨‹å¼‚å¸¸:{e}"
        message_block.set_msg(msg)
    return message_block


def BPE_train_pretokenize(pool, input_path: str, special_tokens: list[str], num_processes: int = 4, ) -> tuple[defaultdict[int], defaultdict[LinkedNode]]:
    """
    å¤šè¿›ç¨‹å®ŒæˆBPEé¢„åˆ†è¯
    """
    # 1. å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ï¼Œè·å–åˆ°æ–‡æ¡£çš„å—è¾¹ç•Œ
    with open(input_path, "rb") as f:
        boundaries = find_chunk_boundaries(f, num_processes, b"<|endoftext|>")
    # 2. ä½¿ç”¨å¤šä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œè¿›è¡Œé¢„åˆ†è¯
    # 2.1 ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä»»åŠ¡
    results = pool.starmap(train_chunk_pretokenize, [(input_path,
                                                      i, boundaries[i:min(i+2, len(boundaries))] if i < len(boundaries)-1 else None,  special_tokens) for i in range(num_processes)])
    # 2.3 ä½¿ç”¨è¿›ç¨‹æ± å°±ä¸éœ€è¦æŒç»­ç›‘å¬æ¶ˆæ¯é˜Ÿåˆ—ï¼Œç›´æ¥éå†resultså³å¯
    all_word_freq = defaultdict(int)
    for message_block in results:
        if message_block.pretokenized_chunks is None:
            msg = f"worker-{message_block.idx} å¤„ç†å¤±è´¥,info:{message_block.msg}"
            logger.info(msg)
        else:
            processed_result = message_block.pretokenized_chunks
            for word in processed_result:
                all_word_freq[word] += processed_result[word]
    # 2.4 ä»æ¯ä¸ªè¿›ç¨‹æ‹¿åˆ°çš„æ˜¯strä»¥åŠå¯¹åº”çš„å‡ºç°é¢‘ç‡ï¼Œä¸Šé¢å·²ç»æ±‡æ€»äº†ç»“æœï¼Œä¸‹é¢éœ€è¦å»ºç«‹èµ·å¯¹åº”çš„åŒå‘é“¾è¡¨æ–¹ä¾¿åç»­merge
    word_list = defaultdict(LinkedNode)
    for word, _ in all_word_freq.items():
        preNode = None
        for byte in word.encode("utf-8"):
            node = LinkedNode()
            node.value = bytes([byte])
            node.preNode = preNode
            if preNode is None:
                word_list[word] = node
            else:
                preNode.nextNode = node
            preNode = node
    return all_word_freq, word_list


def train_BPE(
    input_path: str | os.PathLike,
    vocab_size: int,
    special_tokens: list[str],
    **kwargs,
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """_summary_

    Args:
        input_path (str | os.PathLike): æ•°æ®æ–‡ä»¶è·¯å¾„
        vocab_size (int): è¯è¡¨æœ€å¤§å¤§å°
        special_tokens (list[str]): ç‰¹æ®Štokens

    Returns:
        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]: tokenizerå‚æ•°ï¼Œåˆ†åˆ«ä¸ºè¯è¡¨å’Œmerges
    """
    def updatePair(pair, delta, string):
        """_summary_

        è¾…åŠ©å‡½æ•°ï¼Œä¿®æ”¹pairçš„countä»¥åŠå‡ºç°çš„å­—ç¬¦ä¸² 
        """
        near_bytes_count[pair] += delta
        if delta < 0:
            # æ— è®ºå¦‚ä½•ï¼Œåªè¦ delta < 0ï¼Œè¿™ä¸ª string å°±ä¸å†åŒ…å«è¿™ä¸ª pair
            # æ‰€ä»¥ä» set ä¸­ç§»é™¤å®ƒ
            # è¿™é‡Œæœ‰é—®é¢˜ï¼Œå› ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ä¸­å¯èƒ½å‡ºç°å¤šæ¬¡pairï¼Œå› æ­¤ä¸èƒ½ç›´æ¥å°†å­—ç¬¦ä¸²ä¸¢å¼ƒ
            pair_exist_str[pair][string] -= 1  # è¿™é‡Œå·²ç»åšäº†
        if near_bytes_count[pair] <= 0:
            # å¦‚æœé¢‘ç‡ä¸º0æˆ–è´Ÿæ•°ï¼Œè¯´æ˜è¿™ä¸ª pair å·²ç»ä¸å†æœ‰æ•ˆ
            # æ­¤æ—¶åº”è¯¥ä» near_bytes_count å’Œ pair_exist_str ä¸­å½»åº•ç§»é™¤
            near_bytes_count.pop(pair, None)  # ä½¿ç”¨ .pop(key, None) é˜²æ­¢ KeyErorr
            pair_exist_str.pop(pair, None)  # åŒæ—¶ç§»é™¤ pair_exist_str ä¸­çš„å¯¹åº”é¡¹
        elif delta > 0:
            # åªæœ‰åœ¨ delta > 0 ä¸” pair é¢‘ç‡ä»ç„¶æœ‰æ•ˆæ—¶ï¼Œæ‰æ·»åŠ åˆ° set
            pair_exist_str[pair][string] += 1
        # æ—¥å¿—è®°å½•
        # logger.info(
        #     f"Updated pair {pair}: delta={delta}, pre_count={pre_count},new_count={near_bytes_count.get(pair, 0)}, string={string},string_freq={all_word_freq[string]}")

    start_time = time.time()
    vocab: dict[int, bytes] = {}
    merges: list[tuple[bytes, bytes]] = []
    # 1. åˆå§‹åŒ–è¯è¡¨,ä¸ºäº†å’Œæµ‹è¯•ç”¨ä¾‹ç›¸ç¬¦ï¼Œéœ€è¦è°ƒæ¢é¡ºåºï¼Œå…ˆæ”¾256ä¸ªbytesï¼Œå†æ”¾ç‰¹æ®Štoken
    vocab.update({i: bytes([i]) for i in range(256)})
    vocab_len = len(vocab)
    vocab.update({vocab_len+idx: special_token.encode("utf-8")
                 for idx, special_token in enumerate(special_tokens)})
    # ä½¿ç”¨è¿›ç¨‹æ± é¿å…é‡å¤åˆ›å»ºè¿›ç¨‹å¼€é”€ï¼Œåˆ›å»ºè¿›ç¨‹æ± 
    # åœ¨è¿™é‡Œåˆ›å»ºè¿›ç¨‹æ± ï¼Œå®ƒåªä¼šè¢«åˆ›å»ºä¸€æ¬¡
    num_processes = 4
    with Pool(
        processes=num_processes,
    ) as shared_pool:  # å°†è¿›ç¨‹æ± å‘½åä¸º shared_pool
        # 2. é¢„åˆ†è¯ï¼Œé¢„åˆ†è¯ç»“æœä¸ºæ‰€æœ‰å•è¯çš„é¢‘ç‡ä»¥åŠæ¯ä¸ªå•è¯å¯¹åº”çš„åŒå‘é“¾è¡¨
        start_time = time.time()
        all_word_freq, word_list = BPE_train_pretokenize(
            shared_pool, input_path, special_tokens, num_processes)
        end_time = time.time()
        logger.info(
            f"-----é¢„åˆ†è¯æ—¶é—´ï¼š{end_time-start_time}ï¼Œæ–‡æ¡£å—æ•°ï¼š{len(all_word_freq)}----")
        # 3. ç»Ÿè®¡ç›¸é‚»å­—èŠ‚å¯¹é¢‘ç‡ï¼Œéœ€è¦éå†æ¯ä¸ªå•è¯å¯¹åº”çš„åŒå‘é“¾è¡¨ï¼ŒåŒæ—¶éœ€è¦ç»Ÿè®¡æ¯ä¸ªå­—èŠ‚å¯¹å‡ºç°çš„å•è¯ã€å…³é”®ä¼˜åŒ–ã€‘
        start_time = time.time()
        near_bytes_count = defaultdict(int)
        pair_exist_str = defaultdict(lambda: defaultdict(int))
        for word, head in word_list.items():
            while head.nextNode is not None:
                pair = (head.value, head.nextNode.value)
                near_bytes_count[pair] += all_word_freq[word]
                pair_str_count = pair_exist_str[pair][word]
                pair_exist_str[pair][word] = pair_str_count+1
                head = head.nextNode
        end_time = time.time()
        logger.info(
            f"-----ç›¸é‚»å­—èŠ‚å¯¹ç»Ÿè®¡æ—¶é—´ï¼š{end_time-start_time}----")
        # 4. è¿›è¡Œmerge
        # 4.1 å»ºå †ï¼Œä½¿ç”¨è‡ªå®šä¹‰ç±»å°è£…pairå’Œæ¯”è¾ƒçš„é€»è¾‘ï¼Œä½¿ç”¨å †å–å‡ºæ•°é‡æœ€å¤šçš„pairï¼Œæ³¨æ„å–å‡ºçš„pairå¯èƒ½æ˜¯è¿‡æ—¶çš„ï¼Œéœ€è¦åˆ¤æ–­
        start_time = time.time()
        heap = []
        for pair, freq in near_bytes_count.items():
            item = PairItem(pair=pair, count=freq)
            heapq.heappush(heap, item)
        end_time = time.time()
        logger.info(
            f"-----å»ºå †æ—¶é—´ï¼š{end_time-start_time}----")
        vocab_len = len(vocab)
        nums_to_merge = vocab_size - vocab_len
        start_time = time.time()
        idx = 0
        while idx < nums_to_merge:
            item: PairItem = heapq.heappop(heap)
            pair = item.pair
            # å †ä¸­çš„æ•°æ®å¯èƒ½ä¼šè¿‡æ—¶ï¼Œå› æ­¤å–å‡ºçš„æ—¶å€™éœ€è¦è¿›è¡Œæ¯”å¯¹
            if item.count != near_bytes_count[pair]:
                continue
            # æ²¡è¿‡æœŸå°±è¿›è¡Œmerge
            # 4.2 é’ˆå¯¹ç›¸å…³çš„å•è¯éå†åŒå‘é“¾è¡¨è¿›è¡Œmergeï¼Œmergeè¿‡ç¨‹ä¸­ä¿®æ”¹ç›¸é‚»å­—èŠ‚å¯¹æ•°é‡ï¼Œå­—èŠ‚å¯¹å‡ºç°å­—ç¬¦ä¸²ï¼Œå°†
            # å‡ºç°ä¿®æ”¹çš„pairé‡æ–°æ”¾è¿›å †ä¸­
            new_token = pair[0]+pair[1]
            merges.append(pair)
            vocab[vocab_len+idx] = new_token
            related_strs = list(pair_exist_str[pair].keys())
            for string in related_strs:
                # è¯´æ˜pairåœ¨è¯¥å­—ç¬¦ä¸²ä¸­æ²¡æœ‰å‡ºç°äº†
                if pair_exist_str[pair][string] == 0:
                    continue
                node: LinkedNode = word_list[string]
                str_freq = all_word_freq[string]
                while node is not None and node.nextNode is not None:
                    nextNode = node.nextNode
                    if node.value == pair[0] and nextNode.value == pair[1]:
                        node.value = new_token
                        node.nextNode = nextNode.nextNode
                        # éœ€è¦ä¿®æ”¹çš„å†…å®¹ï¼š1. ç›¸é‚»å­—èŠ‚å¯¹é¢‘ç‡ç»Ÿè®¡è¡¨ 2. å­—èŠ‚å¯¹å‡ºç°å­—ç¬¦ä¸²ç»Ÿè®¡è¡¨ 3. ä¿®æ”¹ä¹‹åé‡æ–°æ”¾è¿›å †ä¸­
                        updatePair(pair, -str_freq, string)
                        if node.preNode is not None:
                            prePair = (node.preNode.value, pair[0])
                            newPair = (node.preNode.value, new_token)
                            updatePair(prePair, -str_freq, string)
                            updatePair(newPair, str_freq, string)
                            heapq.heappush(heap, PairItem(
                                prePair, near_bytes_count[prePair]))
                            heapq.heappush(heap, PairItem(
                                newPair, near_bytes_count[newPair]))
                        if nextNode.nextNode is not None:
                            nextNode.nextNode.preNode = node
                            nexPair = (pair[1], nextNode.nextNode.value)
                            newPair = (new_token, nextNode.nextNode.value)
                            updatePair(nexPair, -str_freq, string)
                            updatePair(newPair, str_freq, string)
                            heapq.heappush(heap, PairItem(
                                nexPair, near_bytes_count[nexPair]))
                            heapq.heappush(heap, PairItem(
                                newPair, near_bytes_count[newPair]))
                        continue
                    node = node.nextNode
            near_bytes_count.pop(pair, 0)
            pair_exist_str.pop(pair, None)
            idx += 1
    end_time = time.time()
    logger.info(
        f"-----mergeæ—¶é—´ï¼š{end_time-start_time}----")
    return (vocab, merges)


@dataclass
class BPETokenizerParam:
    vocab: dict[int, bytes]
    merges: list[tuple[bytes, bytes]]
    special_tokens: list[str] | None = None


class SlowBPETokenizer:
    def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str] | None = None):
        self.vocab = vocab
        self.merges = merges
        if special_tokens is not None:
            self.special_tokens = set(special_tokens)
        else:
            self.special_tokens = set()
        self.stoi = self.reverse_vocab(vocab)

    def reverse_vocab(self,vocab):
        stoi = defaultdict(int)
        for key, value in vocab.items():
            stoi[value] = key
        return stoi

    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: list[str] | None = None):
        """_summary_

        Args:
            vocab_filepath (str): _description_
            merges_filepath (str): _description_
            special_tokens (list[str] | None, optional): _description_. Defaults to None.
        ç”±äºä¸çŸ¥é“æ–‡ä»¶æ ¼å¼å°±å½“æˆjsonæ–‡ä»¶å»è¯»å–
        """
        with open(vocab_filepath, "r", encoding="utf-8") as f:
            import json
            vocab_data = json.load(f)
            # ç¡®ä¿vocabçš„é”®æ˜¯æ•´æ•°ï¼ˆJSONä¼šå°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            cls.vocab = {int(k): v.encode('utf-8') if isinstance(v, str) else v
                         for k, v in vocab_data.items()}
        cls.stoi = cls.reverse_vocab(cls.vocab)

        cls.merges: list[tuple[bytes, bytes]] = []
        with open(merges_filepath, "r", encoding="utf-8") as f1:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                pairs = line.split(" ")
                # å°†ä¸¤ä¸ªtokenè½¬æ¢ä¸ºbytes
                token1, token2 = pairs[0].encode(
                    'utf-8'), pairs[1].encode('utf-8')
                cls.merges.append((token1, token2))

        cls.special_tokens = set(special_tokens)
        return cls

    def pretokenize(self, text) -> list[str]:
        result = []
        for word in re.finditer(
                r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""", text):
            # éœ€è¦åŠ ä¸Šå»æ‰\rï¼Œå¦åˆ™ä¼šå‡ºç°é—®é¢˜
            result.append(word.group(0).replace('\r', ''))
        return result

    def encode(self, text: str) -> list[int]:
        # 1. æŒ‰ç…§special tokenå¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—ï¼Œè¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œå¦‚æœspecial tokensä¸ºç©ºï¼Œé‚£ä¹ˆæ­£åˆ™è¡¨è¾¾å¼åˆ‡åˆ†çš„æ•ˆæœæ˜¯æŒ‰ç…§æ¯ä¸ªè‹±æ–‡å­—æ¯è¿›è¡Œåˆ‡åˆ†
        if len(self.special_tokens)!=0:
            # TODO è¿™é‡Œç”±äº|ä¼šä¼˜å…ˆåŒ¹é…å‰é¢çš„ï¼Œå› æ­¤å¦‚æœ<s>åœ¨<s><s>å‰é¢å°±ä¼šæŠŠ<s><s>æ‹†æˆä¸¤ä¸ªï¼Œå› æ­¤è¿™é‡Œå…ˆæ’åº
            blocks = re.split(
            '(' + '|'.join(map(re.escape, sorted(list(self.special_tokens),key=len,reverse=True))) + ')', text)
        else:
            blocks = [text]
        # 2. é’ˆå¯¹æ¯ä¸ªblockè¿›è¡Œé¢„åˆ†è¯
        # æŒ‰é¡ºåºå­˜æ”¾æ¯ä¸ªå•è¯ä»¥åŠspecial token
        blocks_list = []
        for block in blocks:
            if not block:
                continue
            if block in self.special_tokens:
                blocks_list.append(block)
            else:
                blocks_list.extend(self.pretokenize(block))
        del blocks
        # 3. é¢„åˆ†è¯ä¹‹åå¯¹æ¯ä¸ªblockè½¬æ¢æˆbytesçš„å½¢å¼ï¼Œä¾¿äºmerge
        # ä½¿ç”¨ä¸€ä¸ªå­—å…¸å­˜æ”¾æ‰€æœ‰å•è¯ï¼Œé¿å…é‡å¤è¿ç®—
        word_list = defaultdict(LinkedNode)
        for word in blocks_list:
            if word in self.special_tokens:
                continue
            node: LinkedNode = word_list[word]
            # å¯¹æ¯ä¸ªè¯å»ºç«‹ä¸€ä¸ªå”¯ä¸€çš„é“¾è¡¨
            for byte in word.encode("utf-8"):
                # éå†å­—èŠ‚æ•°ç»„å¾—åˆ°çš„byteçš„å€¼ä¸ºä¸€ä¸ªintå€¼ï¼Œéœ€è¦è½¬æ¢æˆå­—èŠ‚æ•°ç»„å†è¿›è¡Œåç»­æ“ä½œ
                node.value = bytes([byte])
                node.token_id = self.stoi[bytes([byte])]
                newNode = LinkedNode()
                node.nextNode = newNode
                newNode.preNode = node
                node = newNode
            node.preNode.nextNode = None
        # 4. å®Œæˆmergeï¼Œéœ€è¦éå†mergesï¼Œæ¯ä¸ªmergeå¯¹å¯¹æ‰€æœ‰å•è¯è¿›è¡Œmerge
        # å½“å‰ç®—æ³•ä½æ•ˆçš„ç‚¹åœ¨äºå¯¹äºæ¯ä¸ªmergeå¯¹éå†äº†å…¨éƒ¨çš„å•è¯ï¼Œä¸çŸ¥é“åç»­æ˜¯ä¸æ˜¯æœ‰æ›´é«˜æ•ˆçš„åšæ³•æˆ–è€…æœ‰æ›´å¥½çš„æ•°æ®ç»“æ„
        # a.å¯¹äºæ¯ä¸ªpairå•è¯ä¸­ä¸ä¸€å®šæœ‰ï¼Œå¯ä»¥é€šè¿‡æ‹¼æ¥ä¹‹ååˆ¤æ–­wordä¸­æ˜¯å¦å­˜åœ¨æ¥åˆ¤æ–­
        for pair in self.merges:
            for word, head in word_list.items():
                node: LinkedNode = head
                while node is not None and node.nextNode is not None:
                    nextNode = node.nextNode
                    if node.value == pair[0] and nextNode.value == pair[1]:
                        # éœ€è¦å°†è¿™å¯¹è¿›è¡Œmerge
                        newValue = pair[0]+pair[1]
                        # è¿™é‡Œä¸å°å¿ƒæŠŠnextNodeèµ‹å€¼ç»™äº†value(pythonä¸æ£€æŸ¥ç±»å‹)
                        node.value = newValue
                        node.token_id = self.stoi[newValue]
                        node.nextNode = nextNode.nextNode
                        if nextNode.nextNode is not None:
                            nextNode.nextNode.preNode = node
                        continue
                    node = node.nextNode
        # 5. mergeå®Œæˆä¹‹åæ”¶é›†ç»“æœåˆ°åˆ—è¡¨ä¸­
        encoded_result = []
        for word in blocks_list:
            # ç‰¹æ®Štokenç›´æ¥æ·»åŠ 
            if word in self.special_tokens:
                encoded_result.append(self.stoi[word.encode("utf-8")])
            else:
                head: LinkedNode = word_list[word]
                while head is not None:
                    encoded_result.append(head.token_id)
                    head = head.nextNode
        return encoded_result

    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:
        # encode_iterableæ€è·¯æ¯”è¾ƒç®€å•ï¼Œå°±æ˜¯ä¸€ä¸ªä¸ªå•è¯å»ç¼–ç 
        for text in iterable:
            result = self.encode(text) 
            for token_id in result:
                yield token_id

    def decode(self, ids: list[int]) -> str:
        # 1. å°†token IDsè½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        byte_sequence = b''
        for token_id in ids:
            if token_id in self.vocab:
                byte_sequence += self.vocab[token_id]
            else:
                # å¤„ç†æœªçŸ¥token IDï¼Œå¯ä»¥æ·»åŠ æ›¿æ¢å­—èŠ‚æˆ–æŠ›å‡ºé”™è¯¯
                continue

        # 2. å°†å­—èŠ‚åºåˆ—è§£ç ä¸ºå­—ç¬¦ä¸²ï¼Œè‡ªåŠ¨å¤„ç†é”™è¯¯
        try:
            text = byte_sequence.decode('utf-8', errors='replace')
        except Exception as e:
            logger.error(str(e))

        return text



if __name__ == "__main__":
    file_path = "/home/cong/Projs/assignment1-basics/cs336_basics/address.txt"
    with open(file_path,"r",encoding="utf-8") as f:
        content = f.read()
    blank_str = ""
    # unicode_chr = "ğŸ˜Š"
    unicode_chr  = "ğŸ™ƒ"
    ascii_str = "Hello,how are you?"
    from tests.test_tokenizer import VOCAB_PATH,MERGES_PATH,get_tokenizer_from_vocab_merges_path,test_overlapping_special_tokens
    # tokenizer = get_tokenizer_from_vocab_merges_path(
    #     vocab_path=VOCAB_PATH,
    #     merges_path=MERGES_PATH,
    #     special_tokens=["<|endoftext|>", "<|endoftext|><|endoftext|>"],
    # )
    # test_string = "Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>"
    # token_ids = tokenizer.encode(test_string)
    # decoded = tokenizer.decode(token_ids)
    test_overlapping_special_tokens()
